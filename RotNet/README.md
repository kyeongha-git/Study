# 👋 Introduction

 해당 논문은 UNSUPERVISED REPRESENTATION LEARNING BY PREDICTING IMAGE ROTATIONS 입니다.
본 논문의 핵심은 Self-Supervised Learning입니다. Self-Supervised Learning이란, Supervised Learning의 장점인 Semantic Feature Learning, Generalization과 Unsupervised Learning의 장점인 Minimize Labeling Effort를 합친 방식입니다. Label을 데이터로부터 스스로 만들어내고, 이를 통해 Supervised Learning을 진행하여 사전 학습을 진행합니다. 이후, Labeled Data가 있는 Downstream Task에 Fine-Tuning하여 사용합니다.

 본 논문에서는 Self-Supervised Learning의 Pretext Task로 Rotation Prediction을 사용하였으며
기존의 SOTA Unsupervised Learning 모델과 다수의 벤치마크에서 동등하거나 우수한 성능을 보였습니다. 또한, Supervised Learning 모델과의 gap을 크게 줄였다는 점에서 의의가 있습니다.

# 🚀 Presentation

