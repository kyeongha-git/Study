# ğŸ“š Paper Review and Implementation

![Image](https://github.com/user-attachments/assets/fff6081e-1eb1-4289-b7c3-c7e3c80311d9)
This repository contains the code implementations(Pytorch) of the research papers I have read. Along with the code, I will also include detailed comments and annotations to explain my understanding of the concepts and methodologies presented in each paper. My goal is to provide not only the original implementations but also insights and clarifications based on my personal interpretations.

ë³¸ ë…¼ë¬¸ë¦¬ë·°ëŠ” ìˆ˜ì›ëŒ€í•™êµ ë°ì´í„°ê³¼í•™ë¶€ ê¹€ì§„í˜„ êµìˆ˜ë‹˜ê³¼ í•¨ê»˜ ìŠ¤í„°ë””í•œ ë‚´ìš©ì„ ë‚´í¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.
ëª¨ë“  ë‚´ìš©ì€ í™©ê²½í•˜ê°€ ì‘ì„±í•˜ì˜€ìœ¼ë©°, ë°œí‘œë¥¼ í†µí•´ í”¼ë“œë°±ì„ ë°›ìœ¼ë©° ë‚´ìš©ì„ ë³´ì¶©í•˜ì˜€ìŠµë‹ˆë‹¤.

## ğŸ“ Contents  
- ğŸ“– Paper List  
  - ğŸ’» Code Implementations
  - ğŸ—’ï¸ Annotations and Explanations
  - ğŸ“ˆ PPT (Presentation)

### ğŸ“– Paper List

## LLM
- [Attention is All You Need](http://arxiv.org/abs/1706.03762) (Transformer)
  - [code](https://github.com/kyeongha-git/Study/tree/main/Transformer) (Training Dataset: Multi30k)
  - [Explanations](https://kyeongha-blog.tistory.com/entry/Transformer-Attention-Is-All-You-Need)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Transformer) (Please refer to the README file)
- [An Image Is Worth 16 X 16 Words: Transformers For Image Recognition At Scale](http://arxiv.org/abs/2010.11929) (Vision Transformer)
  - [code](https://github.com/kyeongha-git/Study/tree/main/ViT) (Training Dataset: CIFAR-10)
  - [Explanations](https://kyeongha-blog.tistory.com/entry/Vision-Transformer-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/ViT)(Please refer to the README file)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (BERT)
  - [code](https://github.com/kyeongha-git/Study/tree/main/BERT-pytorch)
  - [Explanations](https://kyeongha-blog.tistory.com/entry/LLM-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EA%BC%BC%EA%BC%BC%ED%9E%88)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/BERT-pytorch)(Please refer to the README file)
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/GPT-2)(Please refer to the README file)
- [Language Models are Few-Shot Learners](https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
  - [Explanations](https://kyeongha-blog.tistory.com/entry/GPT-3-Language-Models-are-Few-Shot-Learners-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EA%BC%BC%EA%BC%BC%ED%9E%88)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/GPT-3)(Please refer to the README file)
- [Scaling Laws for Neural Language Models](http://arxiv.org/abs/2001.08361) (Scailng Law)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Scailng%20Law)(Please refer to the README file)

## Basic CNN
- CNN Case Study (AlexNet, VGG, GoogLeNet)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/CNN%20(AlexNet%2CVGG%2CGoogLeNet))(Please refer to the README file)
- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (ResNet)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/ResNet)(Please refer to the README file)

## SSL (self-supervised Learning)
- [UNSUPERVISED REPRESENTATION LEARNING BY PREDICTING IMAGE ROTATIONS](http://arxiv.org/abs/1803.07728) (RotNet)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/RotNet)(Please refer to the README file)
- [A Simple Framework for Contrastive Learning of Visual Representations](http://arxiv.org/abs/2002.05709) (SimCLR)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/SimCLR)(Please refer to the README file)
- [Momentum Contrast for Unsupervised Visual Representation Learning](http://arxiv.org/abs/1911.05722) (MoCo)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/MoCo)((Please refer to the README file)
- [Learning Transferable Visual Models From Natural Language Supervision](http://arxiv.org/abs/2103.00020) (CLIP)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/CLIP)((Please refer to the README file)
