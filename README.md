# VLM Study â€” Paper Reviews & PyTorch Implementations

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Topic](https://img.shields.io/badge/Topics-VLM%20%7C%20LLM%20%7C%20SSL%20%7C%20CNN-lightgrey)
![Last Updated](https://img.shields.io/badge/Last%20Updated-2025--08--13-success)

![Image](https://github.com/user-attachments/assets/fff6081e-1eb1-4289-b7c3-c7e3c80311d9)

ë³¸ ì €ì¥ì†ŒëŠ” Vision-Language Models(VLM)ì„ ì¤‘ì‹¬ìœ¼ë¡œ, í•µì‹¬ ê¸°ì´ˆ ë…¼ë¬¸ë“¤ì„ **ì§ì ‘ êµ¬í˜„(PyTorch)** í•˜ê³  **í•´ì„¤/ì£¼ì„** ë° **ë°œí‘œ ìë£Œ**ë¡œ ì •ë¦¬í•œ ìŠ¤í„°ë”” ê¸°ë¡ì…ë‹ˆë‹¤. êµ¬í˜„ì˜ ì •í™•ì„±ë¿ ì•„ë‹ˆë¼ **ì„¤ê³„ ì˜ë„ì™€ ë°©ë²•ë¡ ì  ë§¥ë½**ì„ í•´ì„í•˜ëŠ” ë° ì´ˆì ì„ ë‘ì—ˆìŠµë‹ˆë‹¤.

> ë³¸ ë…¼ë¬¸ ë¦¬ë·°ëŠ” **ìˆ˜ì›ëŒ€í•™êµ ë°ì´í„°ê³¼í•™ë¶€ ê¹€ì§„í˜„ êµìˆ˜ë‹˜**ê³¼ í•¨ê»˜í•œ ìŠ¤í„°ë”” ë‚´ìš©ì„ í† ëŒ€ë¡œ í•˜ë©°,  
> ëª¨ë“  ì •ë¦¬ì™€ ì½”ë“œëŠ” **í™©ê²½í•˜**ê°€ ì‘ì„±í–ˆê³ , ë°œí‘œì™€ í”¼ë“œë°±ì„ ê±°ì³ ì§€ì† ë³´ì™„í•˜ê³  ìˆìŠµë‹ˆë‹¤.

---

## ğŸ” Why this repo?
- **VLMì˜ ì „í›„ ë§¥ë½**: LLMÂ·CNNÂ·SSLì˜ íë¦„ ì†ì—ì„œ VLMì„ ì´í•´í•˜ê¸° ìœ„í•œ ì „ë‹¨(å‰æ®µ) ê°œë… ì •ë¦¬  
- **ì¬í˜„ì„±ê³¼ ê°€ë…ì„±**: ì› ë…¼ë¬¸ ì˜ë„ë¥¼ í•´ì¹˜ì§€ ì•ŠëŠ” ë²”ìœ„ì—ì„œ **ê¹¨ë—í•œ PyTorch êµ¬í˜„**ê³¼ **í’ë¶€í•œ ì£¼ì„** ì œê³µ  
- **í•™ìŠµÂ·ë°œí‘œ ì¹œí™”**: ê° í´ë”ì— **ë°œí‘œ ìŠ¬ë¼ì´ë“œ(PPT)** ì™€ **ì„¤ëª… ê¸€**ì„ ì—°ê²°í•´ ë¹ ë¥´ê²Œ í›‘ê³  ê¹Šê²Œ ë“¤ì–´ê°ˆ ìˆ˜ ìˆê²Œ êµ¬ì„±

---

## ğŸ“ Contents
- ğŸ“– **Paper List** â€” ì½ì€ ë…¼ë¬¸ê³¼ ì›ë¬¸ ë§í¬  
- ğŸ’» **Code Implementations** â€” PyTorch êµ¬í˜„(ë°ì´í„°ì…‹Â·ì‹¤í—˜ ì¡°ê±´ ëª…ì‹œ)  
- ğŸ—’ï¸ **Annotations & Explanations** â€” ê°œë…/ì•„ì´ë””ì–´ í•´ì„¤, ì„¤ê³„ ì„ íƒì˜ ê·¼ê±°  
- ğŸ“ˆ **Presentation (PPT)** â€” ë°œí‘œìš© ìŠ¬ë¼ì´ë“œ(ê° í´ë” README ì°¸ê³ )

> ì‹¤í–‰ ë°©ë²•ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” **ê° í´ë”ì˜ README** ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

---

## ğŸ“– Paper List

### LLM / Foundation Models
- **Attention Is All You Need (Transformer)**  
  - [Paper](http://arxiv.org/abs/1706.03762)
  - [Code](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/Transformer) *(Dataset: Multi30k)*  
  - [Explanations](https://kyeongha-blog.tistory.com/entry/Transformer-Attention-Is-All-You-Need)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/Transformer) *(See folder README)*

- **An Image Is Worth 16Ã—16 Words (Vision Transformer)**  
  - [Paper](http://arxiv.org/abs/2010.11929)  
  - [Code](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/Vision%20Transformer) *(Dataset: CIFAR-10)*  
  - [Explanations](https://kyeongha-blog.tistory.com/entry/Vision-Transformer-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE) 
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/Vision%20Transformer) *(See folder README)*

- **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**  
  - [Paper](https://arxiv.org/abs/1810.04805)
  - [Code](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/BERT)
  - [Explanations](https://kyeongha-blog.tistory.com/entry/LLM-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EA%BC%BC%EA%BC%BC%ED%9E%88)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/BERT) *(See folder README)*

- **Language Models are Unsupervised Multitask Learners (GPT-2)**  
  - [Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/GPT-2) *(See folder README)*

- **Language Models are Few-Shot Learners (GPT-3)**  
  - [Paper](https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)  
  - [Explanations](https://kyeongha-blog.tistory.com/entry/GPT-3-Language-Models-are-Few-Shot-Learners-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EA%BC%BC%EA%BC%BC%ED%9E%88)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/GPT-3) *(See folder README)*

- **Scaling Laws for Neural Language Models**  
  - [Paper](http://arxiv.org/abs/2001.08361)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/Scailng-Law) *(See folder README)*

### Basic CNN
- **CNN Case Study (AlexNet Â· VGG Â· GoogLeNet)**  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Basic%20CNN/CNN%20(AlexNet%2CVGG%2CGoogLeNet)) *(See folder README)*

- **Deep Residual Learning for Image Recognition (ResNet)**  
  - [Paper](https://arxiv.org/abs/1512.03385)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Basic%20CNN/ResNet) *(See folder README)*

### SSL (Self-Supervised Learning)
- **Unsupervised Representation Learning by Predicting Image Rotations (RotNet)**  
  - [Paper](http://arxiv.org/abs/1803.07728)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/SSL/RotNet) *(See folder README)*

- **SimCLR: A Simple Framework for Contrastive Learning of Visual Representations**  
  - [Paper](http://arxiv.org/abs/2002.05709)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/SSL/SimCLR) *(See folder README)*

- **MoCo: Momentum Contrast for Unsupervised Visual Representation Learning**  
  - [Paper](http://arxiv.org/abs/1911.05722)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/SSL/MoCo) *(See folder README)*

- **CLIP: Learning Transferable Visual Models From Natural Language Supervision**  
  - [Paper](http://arxiv.org/abs/2103.00020)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/SSL/CLIP) *(See folder README)*

---

## ğŸ§­ How to Navigate
- **ì…ë¬¸ì**: *Presentation* â†’ *Explanations* â†’ *Code* ìˆœì„œë¡œ í° ê·¸ë¦¼ë¶€í„°  
- **ì¬í˜„/í™•ì¥**: *Code*ì˜ ì£¼ì„ê³¼ READMEì˜ ì‹¤í—˜ ì¡°ê±´ì„ ë”°ë¼ baseline ì¬í˜„ â†’ ablation ì ìš©  
- **VLM ë§¥ë½í™”**: ViTÂ·SSLÂ·CLIPì„ ì¶•ìœ¼ë¡œ LLM íŒŒíŠ¸ì˜ ìŠ¤ì¼€ì¼ë§ í†µì°°ì„ ì—°ê²°í•´ ì´í•´

---

## ğŸ“¬ Contact
- Author: **í™©ê²½í•˜ (Kyeongha Hwang)**  
- Blog: https://kyeongha-blog.tistory.com  
- GitHub: https://github.com/kyeongha-git
