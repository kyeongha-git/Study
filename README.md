# VLM Study ‚Äî Paper Reviews & PyTorch Implementations

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![Topic](https://img.shields.io/badge/Topics-VLM%20%7C%20LLM%20%7C%20SSL%20%7C%20CNN-lightgrey)
![Last Updated](https://img.shields.io/badge/Last%20Updated-2025--08--13-success)

![Image](https://github.com/user-attachments/assets/fff6081e-1eb1-4289-b7c3-c7e3c80311d9)

Î≥∏ Ï†ÄÏû•ÏÜåÎäî Vision-Language Models(VLM)ÏùÑ Ï§ëÏã¨ÏúºÎ°ú, ÌïµÏã¨ Í∏∞Ï¥à ÎÖºÎ¨∏Îì§ÏùÑ **ÏßÅÏ†ë Íµ¨ÌòÑ(PyTorch)** ÌïòÍ≥† **Ìï¥ÏÑ§/Ï£ºÏÑù** Î∞è **Î∞úÌëú ÏûêÎ£å**Î°ú Ï†ïÎ¶¨Ìïú Ïä§ÌÑ∞Îîî Í∏∞Î°ùÏûÖÎãàÎã§. Íµ¨ÌòÑÏùò Ï†ïÌôïÏÑ±Îøê ÏïÑÎãàÎùº **ÏÑ§Í≥Ñ ÏùòÎèÑÏôÄ Î∞©Î≤ïÎ°†Ï†Å Îß•ÎùΩ**ÏùÑ Ìï¥ÏÑùÌïòÎäî Îç∞ Ï¥àÏ†êÏùÑ ÎëêÏóàÏäµÎãàÎã§.

> Î≥∏ ÎÖºÎ¨∏ Î¶¨Î∑∞Îäî **ÏàòÏõêÎåÄÌïôÍµê Îç∞Ïù¥ÌÑ∞Í≥ºÌïôÎ∂Ä ÍπÄÏßÑÌòÑ ÍµêÏàòÎãò**Í≥º Ìï®ÍªòÌïú Ïä§ÌÑ∞Îîî ÎÇ¥Ïö©ÏùÑ ÌÜ†ÎåÄÎ°ú ÌïòÎ©∞,  
> Î™®Îì† Ï†ïÎ¶¨ÏôÄ ÏΩîÎìúÎäî **Ìô©Í≤ΩÌïò**Í∞Ä ÏûëÏÑ±ÌñàÍ≥†, Î∞úÌëúÏôÄ ÌîºÎìúÎ∞±ÏùÑ Í±∞Ï≥ê ÏßÄÏÜç Î≥¥ÏôÑÌïòÍ≥† ÏûàÏäµÎãàÎã§.

---

## üîé Why this repo?
- **VLMÏùò Ï†ÑÌõÑ Îß•ÎùΩ**: LLM¬∑CNN¬∑SSLÏùò ÌùêÎ¶Ñ ÏÜçÏóêÏÑú VLMÏùÑ Ïù¥Ìï¥ÌïòÍ∏∞ ÏúÑÌïú Ï†ÑÎã®(ÂâçÊÆµ) Í∞úÎÖê Ï†ïÎ¶¨  
- **Ïû¨ÌòÑÏÑ±Í≥º Í∞ÄÎèÖÏÑ±**: Ïõê ÎÖºÎ¨∏ ÏùòÎèÑÎ•º Ìï¥ÏπòÏßÄ ÏïäÎäî Î≤îÏúÑÏóêÏÑú **Íπ®ÎÅóÌïú PyTorch Íµ¨ÌòÑ**Í≥º **ÌíçÎ∂ÄÌïú Ï£ºÏÑù** Ï†úÍ≥µ  
- **ÌïôÏäµ¬∑Î∞úÌëú ÏπúÌôî**: Í∞Å Ìè¥ÎçîÏóê **Î∞úÌëú Ïä¨ÎùºÏù¥Îìú(PPT)** ÏôÄ **ÏÑ§Î™Ö Í∏Ä**ÏùÑ Ïó∞Í≤∞Ìï¥ Îπ†Î•¥Í≤å ÌõëÍ≥† ÍπäÍ≤å Îì§Ïñ¥Í∞à Ïàò ÏûàÍ≤å Íµ¨ÏÑ±

---

## üìù Contents
- üìñ **Paper List** ‚Äî ÏùΩÏùÄ ÎÖºÎ¨∏Í≥º ÏõêÎ¨∏ ÎßÅÌÅ¨  
- üíª **Code Implementations** ‚Äî PyTorch Íµ¨ÌòÑ(Îç∞Ïù¥ÌÑ∞ÏÖã¬∑Ïã§Ìóò Ï°∞Í±¥ Î™ÖÏãú)  
- üóíÔ∏è **Annotations & Explanations** ‚Äî Í∞úÎÖê/ÏïÑÏù¥ÎîîÏñ¥ Ìï¥ÏÑ§, ÏÑ§Í≥Ñ ÏÑ†ÌÉùÏùò Í∑ºÍ±∞  
- üìà **Presentation (PPT)** ‚Äî Î∞úÌëúÏö© Ïä¨ÎùºÏù¥Îìú(Í∞Å Ìè¥Îçî README Ï∞∏Í≥†)

> Ïã§Ìñâ Î∞©Î≤ïÍ≥º ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞Îäî **Í∞Å Ìè¥ÎçîÏùò README** Î•º Ï∞∏Í≥†ÌïòÏÑ∏Ïöî.

---

## üìñ Paper List

### LLM / Foundation Models
- **Attention Is All You Need** (Transformer) 
  - [Paper](http://arxiv.org/abs/1706.03762)  
  - [Code](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/Transformer) *(Dataset: Multi30k)*  
  - [Explanations](https://kyeongha-blog.tistory.com/entry/Transformer-Attention-Is-All-You-Need)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/Transformer) *(See folder README)*

- **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** (BERT) 
  - [Paper](https://arxiv.org/abs/1810.04805)  
  - [Code](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/BERT)  
  - [Explanations](https://kyeongha-blog.tistory.com/entry/LLM-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EA%BC%BC%EA%BC%BC%ED%9E%88)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/BERT) *(See folder README)*

- **Language Models are Unsupervised Multitask Learners** (GPT-2)  
  - [Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/GPT-2) *(See folder README)*

- **Scaling Laws for Neural Language Models**
  - [Paper](http://arxiv.org/abs/2001.08361)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/Scailng-Law) *(See folder README)*

- **Language Models are Few-Shot Learners** (GPT-3)  
  - [Paper](https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)  
  - [Explanations](https://kyeongha-blog.tistory.com/entry/GPT-3-Language-Models-are-Few-Shot-Learners-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EA%BC%BC%EA%BC%BC%ED%9E%88)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/GPT-3) *(See folder README)*

- **An Image Is Worth 16√ó16 Words** (Vision Transformer)  
  - [Paper](http://arxiv.org/abs/2010.11929)  
  - [Code](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/Vision%20Transformer) *(Dataset: CIFAR-10)*  
  - [Explanations](https://kyeongha-blog.tistory.com/entry/Vision-Transformer-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/LLM-Foundation%20Models/Vision%20Transformer) *(See folder README)*


### Basic CNN
- **CNN Case Study (AlexNet ¬∑ VGG ¬∑ GoogLeNet)**  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Basic%20CNN/CNN%20(AlexNet%2CVGG%2CGoogLeNet)) *(See folder README)*

- **Deep Residual Learning for Image Recognition (ResNet)**  
  - [Paper](https://arxiv.org/abs/1512.03385)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Basic%20CNN/ResNet) *(See folder README)*

### SSL (Self-Supervised Learning)
- **Unsupervised Representation Learning by Predicting Image Rotations (RotNet)**  
  - [Paper](http://arxiv.org/abs/1803.07728)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/SSL/RotNet) *(See folder README)*

- **Momentum Contrast for Unsupervised Visual Representation Learning** (MoCo)  
  - [Paper](http://arxiv.org/abs/1911.05722)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/SSL/MoCo) *(See folder README)*

- **SimCLR: A Simple Framework for Contrastive Learning of Visual Representations** (SimCLR)  
  - [Paper](http://arxiv.org/abs/2002.05709)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/SSL/SimCLR) *(See folder README)*

- **CLIP: Learning Transferable Visual Models From Natural Language Supervision** (CLIP)  
  - [Paper](http://arxiv.org/abs/2103.00020)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/SSL/CLIP) *(See folder README)*

- **Emerging Properties in Self-Supervised Vision Transformers** (DINO)  
  - [Paper](http://arxiv.org/abs/2104.14294)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/SSL/DINO) *(See folder README)*

- **Masked Autoencoders Are Scalable Vision Learners** (MAE)  
  - [Paper](http://arxiv.org/abs/2111.06377)  

- **DINOv2: Learning Robust Visual Features without Supervision** (DINO v2)  
  - [Paper](http://arxiv.org/abs/2304.07193)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/SSL/DINO%20v2) *(See folder README)*


### VLM (Vision Language Model)
- **Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**
  - [Paper](http://arxiv.org/abs/1502.03044)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Vision%20Language%20Models/Show_Attend_And_Tell) *(See folder README)*

- **ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks** (ViLBERT)
  - [Paper](http://arxiv.org/abs/1908.02265)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Vision%20Language%20Models/ViLBERT) *(See folder README)*

- **UNITER: UNiversal Image-TExt Representation Learning** (UNITER)
  - [Paper](http://arxiv.org/abs/1909.11740)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Vision%20Language%20Models/UNITER) *(See folder README)*

- **Align before Fuse: Vision and Language Representation Learning with Momentum Distillation** (ALBEF)
  - [Paper](http://arxiv.org/abs/2107.07651)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Vision%20Language%20Models/ALBEF) *(See folder README)*

- **SimVLM: Simple Visual Language Model Pretraining with Weak Supervision** (SimVLM)
  - [Paper](http://arxiv.org/abs/2108.10904)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Vision%20Language%20Models/SimVLM) *(See folder README)*

- **BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation** (BLIP)
  - [Paper](http://arxiv.org/abs/2201.12086)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Vision%20Language%20Models/BLIP) *(See folder README)*

- **Flamingo: a Visual Language Model for Few-Shot Learning** (Flamingo)
  - [Paper](http://arxiv.org/abs/2204.14198)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Vision%20Language%20Models/Flamingo) *(See folder README)*

- **CoCa: Contrastive Captioners are Image-Text Foundation Models** (CoCa)
  - [Paper](http://arxiv.org/abs/2205.01917)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Vision%20Language%20Models/CoCa) *(See folder README)*

- **Training language models to follow instructions with human feedback** (InstructGPT)
  - [Paper](https://arxiv.org/abs/2203.02155)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Vision%20Language%20Models/InstructGPT) *(See folder README)*

- **BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models** (BLIP-2)
  - [Paper](http://arxiv.org/abs/2301.12597)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Vision%20Language%20Models/BLIP-2) *(See folder README)*

- **Visual Instruction Tuning** (LLaVa)
  - [Paper](http://arxiv.org/abs/2304.08485)  
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Vision%20Language%20Models/LLaVa) *(See folder README)*
 
---

## üß≠ How to Navigate
- **ÏûÖÎ¨∏Ïûê**: *Presentation* ‚Üí *Explanations* ‚Üí *Code* ÏàúÏÑúÎ°ú ÌÅ∞ Í∑∏Î¶ºÎ∂ÄÌÑ∞  
- **Ïû¨ÌòÑ/ÌôïÏû•**: *Code*Ïùò Ï£ºÏÑùÍ≥º READMEÏùò Ïã§Ìóò Ï°∞Í±¥ÏùÑ Îî∞Îùº baseline Ïû¨ÌòÑ ‚Üí ablation Ï†ÅÏö©  
- **VLM Îß•ÎùΩÌôî**: ViT¬∑SSL¬∑CLIPÏùÑ Ï∂ïÏúºÎ°ú LLM ÌååÌä∏Ïùò Ïä§ÏºÄÏùºÎßÅ ÌÜµÏ∞∞ÏùÑ Ïó∞Í≤∞Ìï¥ Ïù¥Ìï¥

---

## üì¨ Contact
- Author: **Ìô©Í≤ΩÌïò (Kyeongha Hwang)**  
- Blog: https://kyeongha-blog.tistory.com  
- GitHub: https://github.com/kyeongha-git
