# 📚 Paper Review and Implementation

![Image](https://github.com/user-attachments/assets/fff6081e-1eb1-4289-b7c3-c7e3c80311d9)
This repository contains the code implementations(Pytorch) of the research papers I have read. Along with the code, I will also include detailed comments and annotations to explain my understanding of the concepts and methodologies presented in each paper. My goal is to provide not only the original implementations but also insights and clarifications based on my personal interpretations.

본 논문리뷰는 수원대학교 데이터과학부 김진현 교수님과 함께 스터디한 내용을 내포하고 있습니다.
모든 내용은 황경하가 작성하였으며, 발표를 통해 피드백을 받으며 내용을 보충하였습니다.

## 📝 Contents  
- 📖 Paper List  
  - 💻 Code Implementations
  - 🗒️ Annotations and Explanations
  - 📈 PPT (Presentation)

### 📖 Paper List

## LLM
- [Attention is All You Need](http://arxiv.org/abs/1706.03762) (Transformer)
  - [code](https://github.com/kyeongha-git/Study/tree/main/Transformer) (Training Dataset: Multi30k)
  - [Explanations](https://kyeongha-blog.tistory.com/entry/Transformer-Attention-Is-All-You-Need)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Transformer) (Please refer to the README file)
- [An Image Is Worth 16 X 16 Words: Transformers For Image Recognition At Scale](http://arxiv.org/abs/2010.11929) (Vision Transformer)
  - [code](https://github.com/kyeongha-git/Study/tree/main/ViT) (Training Dataset: CIFAR-10)
  - [Explanations](https://kyeongha-blog.tistory.com/entry/Vision-Transformer-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/ViT)(Please refer to the README file)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (BERT)
  - [code](https://github.com/kyeongha-git/Study/tree/main/BERT-pytorch)
  - [Explanations](https://kyeongha-blog.tistory.com/entry/LLM-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EA%BC%BC%EA%BC%BC%ED%9E%88)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/BERT-pytorch)(Please refer to the README file)
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/GPT-2)(Please refer to the README file)
- [Language Models are Few-Shot Learners](https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
  - [Explanations](https://kyeongha-blog.tistory.com/entry/GPT-3-Language-Models-are-Few-Shot-Learners-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EA%BC%BC%EA%BC%BC%ED%9E%88)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/GPT-3)(Please refer to the README file)
- [Scaling Laws for Neural Language Models](http://arxiv.org/abs/2001.08361) (Scailng Law)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/Scailng%20Law)(Please refer to the README file)

## Basic CNN
- CNN Case Study (AlexNet, VGG, GoogLeNet)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/CNN%20(AlexNet%2CVGG%2CGoogLeNet))(Please refer to the README file)
- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (ResNet)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/ResNet)(Please refer to the README file)

## SSL (self-supervised Learning)
- [UNSUPERVISED REPRESENTATION LEARNING BY PREDICTING IMAGE ROTATIONS](http://arxiv.org/abs/1803.07728) (RotNet)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/RotNet)(Please refer to the README file)
- [A Simple Framework for Contrastive Learning of Visual Representations](http://arxiv.org/abs/2002.05709) (SimCLR)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/SimCLR)(Please refer to the README file)
- [Momentum Contrast for Unsupervised Visual Representation Learning](http://arxiv.org/abs/1911.05722) (MoCo)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/MoCo)((Please refer to the README file)
- [Learning Transferable Visual Models From Natural Language Supervision](http://arxiv.org/abs/2103.00020) (CLIP)
  - [Presentation](https://github.com/kyeongha-git/Study/tree/main/CLIP)((Please refer to the README file)
