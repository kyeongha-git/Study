# ğŸ“š Paper Review and Implementation

This repository contains the code implementations(Pytorch) of the research papers I have read. Along with the code, I will also include detailed comments and annotations to explain my understanding of the concepts and methodologies presented in each paper. My goal is to provide not only the original implementations but also insights and clarifications based on my personal interpretations.

## ğŸ“ Contents  
- ğŸ“– Paper List  
- ğŸ’» Code Implementations
- ğŸ—’ï¸ Annotations and Explanations
- ğŸ“ˆ PPT (Presentation)

### ğŸ“– Paper List
- [Attention is All You Need](http://arxiv.org/abs/1706.03762) (Transformer)
  - [code](https://github.com/kyeongha-git/Study/tree/main/Transformer) (Training Dataset: Multi30k)
- [An Image Is Worth 16 X 16 Words: Transformers For Image Recognition At Scale](http://arxiv.org/abs/2010.11929) (Vision Transformer)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (BERT)

### ğŸ’» Code Implementations
- [Attention is All You Need](https://github.com/kyeongha-git/Study/tree/main/Transformer) (Training Dataset: Multi30k)
- [An Image Is Worth 16 X 16 Words: Transformers For Image Recognition At Scale](https://github.com/kyeongha-git/Study/tree/main/ViT) (Training Dataset: CIFAR-10)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://github.com/kyeongha-git/Study/tree/main/BERT-pytorch)

### ğŸ—’ï¸ Annotations and Explanations (Tistory - Korean)
ğŸŒ [Attention is All You Need](https://kyeongha-blog.tistory.com/entry/Transformer-Attention-Is-All-You-Need)

ğŸŒ [An Image Is Worth 16 X 16 Words: Transformers For Image Recognition At Scale](https://kyeongha-blog.tistory.com/entry/Vision-Transformer-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE)

ğŸŒ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://kyeongha-blog.tistory.com/entry/LLM-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EA%BC%BC%EA%BC%BC%ED%9E%88)

### ğŸ“ˆ PPT (Presentation - Korean)
- [Attention is All You Need]() (Trnasformer)
- [An Image Is Worth 16 X 16 Words: Transformers For Image Recognition At Scale]() [Vision Transformer]
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]() [BERT)
