{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5885a8fe-22f9-401b-a66e-036c4fbff394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, d_model, device):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.device = device\n",
    "\n",
    "        # 패치 분할 후 d_model로 차원 맞추기\n",
    "        self.projection = nn.Linear(patch_size * patch_size * in_channels, d_model)\n",
    "\n",
    "        # [CLS] 토큰\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model, device = device))\n",
    "\n",
    "        # Position Embedding (논문에서 positional encoding을 learnable 하게 만들어서 정하는게 더 적합하다고 나옴.)\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, d_model, device = device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        # 패치로 분할하고 Flatten: (Batch, Num_Patches, Patch_Size * Patch_Size * Channel)\n",
    "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        patches = patches.contiguous().view(batch_size, channels, -1, self.patch_size * self.patch_size)\n",
    "        patches = patches.permute(0, 2, 1, 3).flatten(2)  # (Batch, Num_Patches, Patch_Size * Patch_Size * Channel)\n",
    "\n",
    "        # 선형 변환을 통해 임베딩: (Batch, Num_Patches, d_model)\n",
    "        patch_embeddings = self.projection(patches)\n",
    "\n",
    "        # [CLS] 토큰을 배치 크기만큼 복제하여 앞에 추가\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat((cls_tokens, patch_embeddings), dim=1)  # (Batch, Num_Patches + 1, d_model)\n",
    "\n",
    "        # Position Encoding 추가\n",
    "        embeddings = embeddings + self.position_embedding\n",
    "\n",
    "        return embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
