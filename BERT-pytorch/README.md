# ğŸ‘‹ Introduction

í•´ë‹¹ ë…¼ë¬¸ì€ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ë¼ëŠ” ë…¼ë¬¸ì…ë‹ˆë‹¤.
BERT ëª¨ë¸ì€ Transformerì˜ Encoder êµ¬ì¡°ë¥¼ í™œìš©í•´ ë§Œë“  Language Representation Modelë¡œ, ë‘ ê°€ì§€ íƒœìŠ¤í¬ë¡œ Pre-Trainingì„ í•œ ëª¨ë¸ì…ë‹ˆë‹¤.
ì—¬ëŸ¬ ì–¸ì–´ íƒœìŠ¤í¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

# ğŸš€ Presentation

