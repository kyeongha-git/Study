# 👋 Introduction

해당 논문은 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 라는 논문입니다.
BERT 모델은 Transformer의 Encoder 구조를 활용해 만든 Language Representation Model로, 두 가지 태스크로 Pre-Training을 한 모델입니다.
여러 언어 태스크에서 우수한 성능을 보입니다.

# 🚀 Presentation

